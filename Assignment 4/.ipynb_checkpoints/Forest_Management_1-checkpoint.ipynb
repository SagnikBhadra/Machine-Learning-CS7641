{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "52c63455",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hiive.mdptoolbox.mdp import ValueIteration, PolicyIteration, QLearning\n",
    "from hiive.mdptoolbox.example import forest\n",
    "# import hiive_mdptoolbox.example\n",
    "# import hiive_mdptoolbox\n",
    "import gym\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from numpy.random import choice\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from util import provide_scores, adjust_data_structure, show_decisions, tsting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a07366e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 States\n",
      "\n",
      "\n",
      "\n",
      "        Epsilon                                             Policy Iteration  \\\n",
      "0  1.000000e-01  (0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        33   \n",
      "1  1.000000e-03  (0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        55   \n",
      "2  1.000000e-06  (0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        87   \n",
      "3  1.000000e-09  (0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       120   \n",
      "4  1.000000e-12  (0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       153   \n",
      "5  1.000000e-15  (0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       186   \n",
      "\n",
      "       Time    Reward                                     Value Function  \n",
      "0  0.002859  2.843259  (4.328504830081768, 4.881518644971712, 4.88151...  \n",
      "1  0.002134  2.842526  (4.460720290173723, 5.013211594807497, 5.01321...  \n",
      "2  0.003557  2.920512  (4.474643139169861, 5.027129333047953, 5.02712...  \n",
      "3  0.004605  2.884589  (4.475122825121185, 5.027609012960728, 5.02760...  \n",
      "4  0.005545  2.885044  (4.475137648839068, 5.027623836684378, 5.02762...  \n",
      "5  0.007054  2.913634  (4.4751381069387985, 5.027624294784101, 5.0276...  \n",
      "14 0.006145000457763672 2.8669479098730797\n",
      "(0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "Q-Learning\n",
      "1: 3.2419625985195655\n",
      "2: 3.45468712711018\n",
      "3: 2.9725909845507967\n",
      "4: 3.1199088702065234\n",
      "5: 1.1\n",
      "6: 3.4605589330321034\n",
      "7: 3.1957922700013963\n",
      "8: 0.8\n",
      "9: 3.3193457298294207\n",
      "10: 3.358040492544265\n",
      "11: 3.052963918650888\n",
      "12: 3.165223845408274\n",
      "13: 3.4057947654678107\n",
      "14: 3.4913868295783455\n",
      "15: 3.200831840749126\n",
      "16: 3.1403480871645533\n",
      "17: 3.3697538256736776\n",
      "18: 3.3341063241755533\n",
      "19: 2.8207445098572754\n",
      "20: 0.85\n",
      "21: 0.95\n",
      "22: 3.486291960372364\n",
      "23: 3.271429634595206\n",
      "24: 0.95\n",
      "25: 3.1392181548026974\n",
      "26: 3.4049870182429904\n",
      "27: 3.1145079958705306\n",
      "28: 2.9350139196004505\n",
      "29: 3.455509245021752\n",
      "30: 3.4166385707092233\n",
      "31: 3.3223197902165595\n",
      "32: 1.0\n",
      "0    True\n",
      "1    True\n",
      "2    True\n",
      "3    True\n",
      "4    True\n",
      "5    True\n",
      "Name: Policy, dtype: bool\n",
      "   Iterations  Alpha Decay  Alpha Min  Epsilon  Epsilon Decay    Reward  \\\n",
      "0     1000000        0.990     0.0010     10.0          0.990  3.241963   \n",
      "1     1000000        0.990     0.0001     10.0          0.990  3.454687   \n",
      "2     1000000        0.999     0.0010     10.0          0.990  2.972591   \n",
      "3     1000000        0.999     0.0001     10.0          0.990  3.119909   \n",
      "4     1000000        0.990     0.0010     10.0          0.999  1.100000   \n",
      "5     1000000        0.990     0.0001     10.0          0.999  3.460559   \n",
      "6     1000000        0.999     0.0010     10.0          0.999  3.195792   \n",
      "7     1000000        0.999     0.0001     10.0          0.999  0.800000   \n",
      "8     1000000        0.990     0.0010      1.0          0.990  3.319346   \n",
      "9     1000000        0.990     0.0001      1.0          0.990  3.358040   \n",
      "10    1000000        0.999     0.0010      1.0          0.990  3.052964   \n",
      "11    1000000        0.999     0.0001      1.0          0.990  3.165224   \n",
      "12    1000000        0.990     0.0010      1.0          0.999  3.405795   \n",
      "13    1000000        0.990     0.0001      1.0          0.999  3.491387   \n",
      "14    1000000        0.999     0.0010      1.0          0.999  3.200832   \n",
      "15    1000000        0.999     0.0001      1.0          0.999  3.140348   \n",
      "16   10000000        0.990     0.0010     10.0          0.990  3.369754   \n",
      "17   10000000        0.990     0.0001     10.0          0.990  3.334106   \n",
      "18   10000000        0.999     0.0010     10.0          0.990  2.820745   \n",
      "19   10000000        0.999     0.0001     10.0          0.990  0.850000   \n",
      "20   10000000        0.990     0.0010     10.0          0.999  0.950000   \n",
      "21   10000000        0.990     0.0001     10.0          0.999  3.486292   \n",
      "22   10000000        0.999     0.0010     10.0          0.999  3.271430   \n",
      "23   10000000        0.999     0.0001     10.0          0.999  0.950000   \n",
      "24   10000000        0.990     0.0010      1.0          0.990  3.139218   \n",
      "25   10000000        0.990     0.0001      1.0          0.990  3.404987   \n",
      "26   10000000        0.999     0.0010      1.0          0.990  3.114508   \n",
      "27   10000000        0.999     0.0001      1.0          0.990  2.935014   \n",
      "28   10000000        0.990     0.0010      1.0          0.999  3.455509   \n",
      "29   10000000        0.990     0.0001      1.0          0.999  3.416639   \n",
      "30   10000000        0.999     0.0010      1.0          0.999  3.322320   \n",
      "31   10000000        0.999     0.0001      1.0          0.999  1.000000   \n",
      "\n",
      "            Time                                             Policy  \\\n",
      "0      33.040414  (0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, ...   \n",
      "1      29.519516  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, ...   \n",
      "2      29.233456  (0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, ...   \n",
      "3      28.874018  (0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, ...   \n",
      "4      29.100523  (0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "5      28.886386  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "6      28.830022  (0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, ...   \n",
      "7      28.736845  (0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
      "8      28.791635  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, ...   \n",
      "9      28.696236  (0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, ...   \n",
      "10     28.545516  (0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, ...   \n",
      "11     28.597038  (0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, ...   \n",
      "12     28.612577  (0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, ...   \n",
      "13     28.926453  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "14     29.522075  (0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, ...   \n",
      "15     30.562320  (0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, ...   \n",
      "16    294.180137  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "17    287.980783  (0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "18    286.550121  (0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
      "19  24111.604362  (0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, ...   \n",
      "20    314.293222  (0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, ...   \n",
      "21    361.233167  (0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "22    379.910391  (0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, ...   \n",
      "23    328.321019  (0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, ...   \n",
      "24    316.390632  (0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, ...   \n",
      "25    353.193523  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, ...   \n",
      "26    354.763767  (0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, ...   \n",
      "27    319.916854  (0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, ...   \n",
      "28    298.062238  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "29    316.663202  (0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "30    306.335679  (0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, ...   \n",
      "31    318.935659  (0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, ...   \n",
      "\n",
      "                                       Value Function  \\\n",
      "0   (4.469516616179936, 5.021068518048445, 5.02559...   \n",
      "1   (4.439242057739486, 4.991535438072977, 4.02348...   \n",
      "2   (4.484346439269151, 5.034010306485361, 5.03050...   \n",
      "3   (4.472149848246118, 5.025254217665293, 4.80726...   \n",
      "4   (4.475160791588194, 5.030119252396098, 5.03305...   \n",
      "5   (4.4320373705496445, 4.983831695810947, 4.0022...   \n",
      "6   (4.4755272438345965, 5.026659913734769, 5.0244...   \n",
      "7   (4.47387595179908, 5.02543011236457, 4.8995375...   \n",
      "8   (4.4715302899745675, 5.025254103920376, 5.0260...   \n",
      "9   (4.440501282624428, 4.9917634204759755, 4.0250...   \n",
      "10  (4.473243669077801, 5.024557765574801, 5.02822...   \n",
      "11  (4.474735106643947, 5.026734368971076, 4.79515...   \n",
      "12  (4.468336410499357, 5.024880214155668, 5.02617...   \n",
      "13  (4.437712559977625, 4.989278727892473, 4.05522...   \n",
      "14  (4.483947176287941, 5.027209497405266, 5.02235...   \n",
      "15  (4.4733679982784, 5.027344760662783, 4.9210824...   \n",
      "16  (4.478727097439459, 5.030297107574103, 5.02726...   \n",
      "17  (4.472545747925688, 5.026882228828553, 4.47434...   \n",
      "18  (4.487381826567785, 5.036270669291105, 5.03398...   \n",
      "19  (4.475654838515648, 5.027937255853335, 5.02832...   \n",
      "20  (4.478315670860931, 5.030859434325373, 5.03174...   \n",
      "21  (4.474343961653026, 5.026747473331291, 5.02772...   \n",
      "22  (4.47385368370041, 5.02501370437887, 5.0223674...   \n",
      "23  (4.473700196315319, 5.026368493160579, 5.02740...   \n",
      "24  (4.473987946896234, 5.027548079374795, 5.02555...   \n",
      "25  (4.473806835083367, 5.026323456779912, 5.02747...   \n",
      "26  (4.469272793002597, 5.021617837049628, 5.02717...   \n",
      "27  (4.470958536828379, 5.023586130976227, 5.02661...   \n",
      "28  (4.469424831306316, 5.023062253352429, 5.02330...   \n",
      "29  (4.472547871818648, 5.025404751471131, 5.02714...   \n",
      "30  (4.4748960340209925, 5.028188398473192, 5.0310...   \n",
      "31  (4.47285829170428, 5.025951108926845, 5.026750...   \n",
      "\n",
      "                                     Training Rewards  \n",
      "0   [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
      "1   [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, ...  \n",
      "2   [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "3   [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...  \n",
      "4   [6.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, ...  \n",
      "5   [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...  \n",
      "6   [10.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0,...  \n",
      "7   [0.0, 0.0, 0.0, 0.0, 1.0, 10.0, 0.0, 0.0, 0.0,...  \n",
      "8   [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 10.0, 0.0, 0.0,...  \n",
      "9   [0.0, 10.0, 0.0, 0.0, 10.0, 0.0, 0.0, 0.0, 1.0...  \n",
      "10  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
      "11  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
      "12  [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, ...  \n",
      "13  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "14  [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "15  [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...  \n",
      "16  [1.0, 0.0, 0.0, 1.0, 10.0, 1.0, 1.0, 1.0, 1.0,...  \n",
      "17  [0.0, 0.0, 1.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
      "18  [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "19  [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, ...  \n",
      "20  [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...  \n",
      "21  [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 10.0, 1.0, 0.0,...  \n",
      "22  [1.0, 0.0, 0.0, 10.0, 0.0, 0.0, 0.0, 1.0, 0.0,...  \n",
      "23  [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, ...  \n",
      "24  [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, ...  \n",
      "25  [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, ...  \n",
      "26  [0.0, 0.0, 1.0, 10.0, 10.0, 0.0, 1.0, 1.0, 1.0...  \n",
      "27  [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
      "28  [0.0, 10.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...  \n",
      "29  [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 10.0, 0.0, 1.0,...  \n",
      "30  [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...  \n",
      "31  [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, ...  \n",
      "            Alpha Decay  Alpha Min  Epsilon  Epsilon Decay    Reward  \\\n",
      "Iterations                                                             \n",
      "1000000          0.9945    0.00055      5.5         0.9945  2.967465   \n",
      "10000000         0.9945    0.00055      5.5         0.9945  2.676283   \n",
      "\n",
      "                   Time  \n",
      "Iterations               \n",
      "1000000       29.279689  \n",
      "10000000    1809.270922  \n",
      "               Alpha Decay  Alpha Min  Epsilon    Reward         Time\n",
      "Epsilon Decay                                                        \n",
      "0.990               0.9945    0.00055      5.5  3.040816  1659.992375\n",
      "0.999               0.9945    0.00055      5.5  2.602931   178.558236\n",
      "500 States\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Epsilon                                             Policy Iteration  \\\n",
      "0  1.000000e-01  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...        79   \n",
      "1  1.000000e-03  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       119   \n",
      "2  1.000000e-06  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       179   \n",
      "3  1.000000e-09  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       239   \n",
      "4  1.000000e-12  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       299   \n",
      "5  1.000000e-15  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       349   \n",
      "\n",
      "       Time    Reward                                     Value Function  \n",
      "0  0.006847  2.725382  (4.710556185449387, 5.239434944489701, 5.23943...  \n",
      "1  0.009498  2.726020  (4.7117745667154995, 5.240595870281114, 5.2405...  \n",
      "2  0.014406  2.750285  (4.711792669916437, 5.240613400253226, 5.24061...  \n",
      "3  0.019457  2.755154  (4.711792702216012, 5.240613431989174, 5.24061...  \n",
      "4  0.023710  2.735265  (4.711792702273827, 5.240613432046434, 5.24061...  \n",
      "5  0.027656  2.743204  (4.7117927022739305, 5.240613432046538, 5.2406...  \n",
      "46 0.21545004844665527 2.7282127846897812\n",
      "(0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "Q-Learning\n",
      "1: 2.6457202687964307\n",
      "2: 2.649979984464296\n",
      "3: 2.6392875993822655\n",
      "4: 2.6315063673157337\n",
      "5: 2.6334844975422964\n",
      "6: 2.662812261587241\n",
      "7: 2.579535594724274\n",
      "8: 2.6408281271978216\n",
      "9: 2.615333529405131\n",
      "10: 0.854\n",
      "11: 2.6253267906693583\n",
      "12: 2.622869833551158\n",
      "13: 2.64128539283331\n",
      "14: 2.6388866548579846\n",
      "15: 2.6272875183965785\n",
      "16: 2.621154178848485\n",
      "17: 2.753957755761328\n",
      "18: 2.8307366870778288\n",
      "19: 2.770383137892614\n",
      "20: 2.8005122228172747\n",
      "21: 2.7597817847014805\n",
      "22: 2.8389751044306744\n",
      "23: 2.7384627338584178\n",
      "24: 2.8055208965447593\n",
      "25: 2.750629574729021\n",
      "26: 2.8216984597515604\n",
      "27: 2.758731007363096\n",
      "28: 2.7894710568318906\n",
      "29: 2.7627961451992804\n",
      "30: 2.831902914125298\n",
      "31: 2.766254934955373\n",
      "32: 2.781121279609314\n",
      "0    True\n",
      "1    True\n",
      "2    True\n",
      "3    True\n",
      "4    True\n",
      "5    True\n",
      "Name: Policy, dtype: bool\n",
      "   Iterations  Alpha Decay  Alpha Min  Epsilon  Epsilon Decay    Reward  \\\n",
      "0     1000000        0.990     0.0010     10.0          0.990  2.645720   \n",
      "1     1000000        0.990     0.0001     10.0          0.990  2.649980   \n",
      "2     1000000        0.999     0.0010     10.0          0.990  2.639288   \n",
      "3     1000000        0.999     0.0001     10.0          0.990  2.631506   \n",
      "4     1000000        0.990     0.0010     10.0          0.999  2.633484   \n",
      "5     1000000        0.990     0.0001     10.0          0.999  2.662812   \n",
      "6     1000000        0.999     0.0010     10.0          0.999  2.579536   \n",
      "7     1000000        0.999     0.0001     10.0          0.999  2.640828   \n",
      "8     1000000        0.990     0.0010      1.0          0.990  2.615334   \n",
      "9     1000000        0.990     0.0001      1.0          0.990  0.854000   \n",
      "10    1000000        0.999     0.0010      1.0          0.990  2.625327   \n",
      "11    1000000        0.999     0.0001      1.0          0.990  2.622870   \n",
      "12    1000000        0.990     0.0010      1.0          0.999  2.641285   \n",
      "13    1000000        0.990     0.0001      1.0          0.999  2.638887   \n",
      "14    1000000        0.999     0.0010      1.0          0.999  2.627288   \n",
      "15    1000000        0.999     0.0001      1.0          0.999  2.621154   \n",
      "16   10000000        0.990     0.0010     10.0          0.990  2.753958   \n",
      "17   10000000        0.990     0.0001     10.0          0.990  2.830737   \n",
      "18   10000000        0.999     0.0010     10.0          0.990  2.770383   \n",
      "19   10000000        0.999     0.0001     10.0          0.990  2.800512   \n",
      "20   10000000        0.990     0.0010     10.0          0.999  2.759782   \n",
      "21   10000000        0.990     0.0001     10.0          0.999  2.838975   \n",
      "22   10000000        0.999     0.0010     10.0          0.999  2.738463   \n",
      "23   10000000        0.999     0.0001     10.0          0.999  2.805521   \n",
      "24   10000000        0.990     0.0010      1.0          0.990  2.750630   \n",
      "25   10000000        0.990     0.0001      1.0          0.990  2.821698   \n",
      "26   10000000        0.999     0.0010      1.0          0.990  2.758731   \n",
      "27   10000000        0.999     0.0001      1.0          0.990  2.789471   \n",
      "28   10000000        0.990     0.0010      1.0          0.999  2.762796   \n",
      "29   10000000        0.990     0.0001      1.0          0.999  2.831903   \n",
      "30   10000000        0.999     0.0010      1.0          0.999  2.766255   \n",
      "31   10000000        0.999     0.0001      1.0          0.999  2.781121   \n",
      "\n",
      "          Time                                             Policy  \\\n",
      "0    43.287664  (0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, ...   \n",
      "1    46.268103  (0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, ...   \n",
      "2    44.387172  (0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, ...   \n",
      "3    44.614347  (0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, ...   \n",
      "4    49.729480  (0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, ...   \n",
      "5    50.667966  (0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, ...   \n",
      "6    53.931024  (0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, ...   \n",
      "7    51.625359  (0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, ...   \n",
      "8    49.303301  (0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "9    52.793726  (0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, ...   \n",
      "10   48.013069  (0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n",
      "11   47.709943  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, ...   \n",
      "12   46.684619  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, ...   \n",
      "13   48.194980  (0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, ...   \n",
      "14   46.698047  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, ...   \n",
      "15   45.576298  (0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, ...   \n",
      "16  448.273037  (0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "17  443.004014  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "18  447.808124  (0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "19  447.223058  (0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "20  445.785576  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, ...   \n",
      "21  444.143030  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "22  447.890250  (0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n",
      "23  457.028816  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "24  458.668420  (0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "25  433.086630  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "26  438.579375  (0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "27  433.420595  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, ...   \n",
      "28  436.940749  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, ...   \n",
      "29  431.681213  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "30  444.232602  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, ...   \n",
      "31  444.908620  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                       Value Function  \\\n",
      "0   (4.711608976583683, 5.240736056301092, 5.24235...   \n",
      "1   (4.672435445357042, 5.201171633515619, 4.37127...   \n",
      "2   (4.7127953406989, 5.240646359078045, 5.2397861...   \n",
      "3   (4.709399081256882, 5.2385485235715645, 5.0989...   \n",
      "4   (4.710522511015649, 5.239877278961145, 5.23978...   \n",
      "5   (4.667861470968527, 5.196229858663855, 4.36511...   \n",
      "6   (4.713127913126084, 5.24227256659081, 5.241421...   \n",
      "7   (4.708568498848688, 5.237654220967237, 5.13409...   \n",
      "8   (4.709616563111147, 5.239579907160608, 5.24055...   \n",
      "9   (4.671803722165624, 5.200420854442883, 4.37439...   \n",
      "10  (4.708798041638176, 5.237348262835154, 5.23897...   \n",
      "11  (4.710611883592539, 5.239766727493796, 5.07795...   \n",
      "12  (4.71104961560227, 5.240555934341656, 5.240119...   \n",
      "13  (4.667983982759229, 5.196549386529219, 4.33630...   \n",
      "14  (4.71420263693811, 5.241776361592537, 5.241448...   \n",
      "15  (4.70893265533937, 5.237969010334612, 5.135133...   \n",
      "16  (4.715240039024286, 5.243235776603333, 5.24187...   \n",
      "17  (4.711408241514055, 5.240331992175588, 5.24084...   \n",
      "18  (4.710333198733517, 5.238904513109211, 5.24051...   \n",
      "19  (4.7112519988621555, 5.24024930807696, 5.24036...   \n",
      "20  (4.714717195027458, 5.242958034955538, 5.24098...   \n",
      "21  (4.711973921873225, 5.240576948153227, 5.24040...   \n",
      "22  (4.712788341687959, 5.240693773673025, 5.23930...   \n",
      "23  (4.711497235915598, 5.2403455492798106, 5.2407...   \n",
      "24  (4.7103739865486585, 5.239823353197184, 5.2408...   \n",
      "25  (4.712662839054791, 5.241339604775162, 5.24059...   \n",
      "26  (4.713983234416544, 5.242644688779662, 5.24156...   \n",
      "27  (4.711567622550278, 5.240700337742658, 5.24075...   \n",
      "28  (4.708659551509187, 5.237944930089279, 5.23920...   \n",
      "29  (4.711861786864244, 5.240179461218474, 5.24015...   \n",
      "30  (4.710272547721519, 5.2395490115711025, 5.2405...   \n",
      "31  (4.7117728053645465, 5.240668420637552, 5.2405...   \n",
      "\n",
      "                                     Training Rewards  \n",
      "0   [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "1   [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "2   [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...  \n",
      "3   [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
      "4   [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, ...  \n",
      "5   [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "6   [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "7   [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "8   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
      "9   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "10  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "11  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
      "12  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "13  [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "14  [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "15  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "16  [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
      "17  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "18  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "19  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "20  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
      "21  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 100.0...  \n",
      "22  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "23  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
      "24  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
      "25  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
      "26  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
      "27  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "28  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
      "29  [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
      "30  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
      "31  [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "            Alpha Decay  Alpha Min  Epsilon  Epsilon Decay    Reward  \\\n",
      "Iterations                                                             \n",
      "1000000          0.9945    0.00055      5.5         0.9945  2.520581   \n",
      "10000000         0.9945    0.00055      5.5         0.9945  2.785058   \n",
      "\n",
      "                  Time  \n",
      "Iterations              \n",
      "1000000      48.092819  \n",
      "10000000    443.917132  \n",
      "               Alpha Decay  Alpha Min  Epsilon    Reward        Time\n",
      "Epsilon Decay                                                       \n",
      "0.990               0.9945    0.00055      5.5  2.597509  245.402536\n",
      "0.999               0.9945    0.00055      5.5  2.708131  246.607414\n"
     ]
    }
   ],
   "source": [
    "def running_mean(x, N):\n",
    "    cumsum = np.cumsum(np.insert(x, 0, 0)) \n",
    "    return (cumsum[N:] - cumsum[:-N]) / float(N)\n",
    "\n",
    "\n",
    "def test_policy(P, R, policy, test_count=1000, gamma=0.9):\n",
    "    num_state = P.shape[-1]\n",
    "    total_episode = num_state * test_count\n",
    "    # start in each state\n",
    "    total_reward = 0\n",
    "    for state in range(num_state):\n",
    "        state_reward = 0\n",
    "        for state_episode in range(test_count):\n",
    "            episode_reward = 0\n",
    "            disc_rate = 1\n",
    "            while True:\n",
    "                # take step\n",
    "                action = policy[state]\n",
    "                # get next step using P\n",
    "                probs = P[action][state]\n",
    "                candidates = list(range(len(P[action][state])))\n",
    "                next_state =  choice(candidates, 1, p=probs)[0]\n",
    "                # get the reward\n",
    "                reward = R[state][action] * disc_rate\n",
    "                episode_reward += reward\n",
    "                # when go back to 0 ended\n",
    "                disc_rate *= gamma\n",
    "                if next_state == 0:\n",
    "                    break\n",
    "            state_reward += episode_reward\n",
    "        total_reward += state_reward\n",
    "    return total_reward / total_episode\n",
    "\n",
    "def trainVI(P, R, discount=0.9, epsilon=[1e-9]):\n",
    "    vi_df = pd.DataFrame(columns=[\"Epsilon\", \"Policy\", \"Iteration\", \n",
    "                                  \"Time\", \"Reward\", \"Value Function\"])\n",
    "    for eps in epsilon:\n",
    "        vi = ValueIteration(P, R, gamma=discount, epsilon=eps, max_iter=int(1e15))\n",
    "        vi.run()\n",
    "        reward = test_policy(P, R, vi.policy)\n",
    "        info = [float(eps), vi.policy, vi.iter, vi.time, reward, vi.V]\n",
    "        df_length = len(vi_df)\n",
    "        vi_df.loc[df_length] = info\n",
    "    return vi_df\n",
    "\n",
    "def trainQ(P, R, discount=0.9, alpha_dec=[0.99], alpha_min=[0.001], \n",
    "            epsilon=[1.0], epsilon_decay=[0.99], n_iter=[1000000]):\n",
    "    q_df = pd.DataFrame(columns=[\"Iterations\", \"Alpha Decay\", \"Alpha Min\", \n",
    "                                 \"Epsilon\", \"Epsilon Decay\", \"Reward\",\n",
    "                                 \"Time\", \"Policy\", \"Value Function\",\n",
    "                                 \"Training Rewards\"])\n",
    "    \n",
    "    count = 0\n",
    "    for i in n_iter:\n",
    "        for eps in epsilon:\n",
    "            for eps_dec in epsilon_decay:\n",
    "                for a_dec in alpha_dec:\n",
    "                    for a_min in alpha_min:\n",
    "                        q = QLearning(P, R, discount, alpha_decay=a_dec, \n",
    "                                      alpha_min=a_min, epsilon=eps, \n",
    "                                      epsilon_decay=eps_dec, n_iter=i)\n",
    "                        q.run()\n",
    "                        reward = test_policy(P, R, q.policy)\n",
    "                        count += 1\n",
    "                        print(\"{}: {}\".format(count, reward))\n",
    "                        st = q.run_stats\n",
    "                        rews = [s['Reward'] for s in st]\n",
    "                        info = [i, a_dec, a_min, eps, eps_dec, reward, \n",
    "                                q.time, q.policy, q.V, rews]\n",
    "                        \n",
    "                        df_length = len(q_df)\n",
    "                        q_df.loc[df_length] = info\n",
    "    return q_df\n",
    "\n",
    "\n",
    "def run_forest_management(P, R):\n",
    "    vi_df = trainVI(P, R, epsilon=[1e-1, 1e-3, 1e-6, 1e-9, 1e-12, 1e-15])\n",
    "    print(vi_df)\n",
    "    \n",
    "    pi = PolicyIteration(P, R, gamma=0.9, max_iter=1e6)\n",
    "    pi.run()\n",
    "    pi_pol = pi.policy\n",
    "    pi_reward = test_policy(P, R, pi_pol)\n",
    "    pi_iter = pi.iter\n",
    "    pi_time = pi.time\n",
    "    print(pi_iter, pi_time, pi_reward)\n",
    "    \n",
    "    print(pi_pol)\n",
    "    \n",
    "    \n",
    "    print(\"Q-Learning\")\n",
    "    \n",
    "    alpha_decs = [0.99, 0.999]\n",
    "    alpha_mins =[0.001, 0.0001]\n",
    "    eps = [10.0, 1.0]\n",
    "    eps_dec = [0.99, 0.999]\n",
    "    iters = [1000000, 10000000]\n",
    "    q_df = trainQ(P, R, discount=0.9, alpha_dec=alpha_decs, alpha_min=alpha_mins, \n",
    "                epsilon=eps, epsilon_decay=eps_dec, n_iter=iters)\n",
    "    \n",
    "    print(vi_df.Policy == pi_pol)\n",
    "    \n",
    "    test_policy(P,R,q_df.Policy[18])\n",
    "    \n",
    "    print(q_df)\n",
    "    \n",
    "    print(q_df.groupby(\"Iterations\").mean())\n",
    "    \n",
    "    print(q_df.groupby(\"Epsilon Decay\").mean())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(44)\n",
    "    \n",
    "    print(\"20 States\\n\\n\\n\")\n",
    "    \n",
    "    P, R = forest(S=20, r1=10, r2=6, p=0.1)\n",
    "\n",
    "    run_forest_management(P, R)\n",
    "    \n",
    "    print(\"500 States\\n\\n\\n\")\n",
    "    \n",
    "    P, R = forest(S=500, r1=100, r2= 15, p=0.01)\n",
    "    \n",
    "    run_forest_management(P, R)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a5b97ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 States\n",
      "\n",
      "\n",
      "\n",
      "        Epsilon                                             Policy Iteration  \\\n",
      "0  1.000000e-01  (0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        33   \n",
      "1  1.000000e-03  (0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        55   \n",
      "2  1.000000e-06  (0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...        87   \n",
      "3  1.000000e-09  (0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       120   \n",
      "4  1.000000e-12  (0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       153   \n",
      "5  1.000000e-15  (0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       186   \n",
      "\n",
      "       Time    Reward                                     Value Function  \n",
      "0  0.004908  2.843259  (4.328504830081768, 4.881518644971712, 4.88151...  \n",
      "1  0.002211  2.842526  (4.460720290173723, 5.013211594807497, 5.01321...  \n",
      "2  0.003582  2.920512  (4.474643139169861, 5.027129333047953, 5.02712...  \n",
      "3  0.005312  2.884589  (4.475122825121185, 5.027609012960728, 5.02760...  \n",
      "4  0.006297  2.885044  (4.475137648839068, 5.027623836684378, 5.02762...  \n",
      "5  0.007634  2.913634  (4.4751381069387985, 5.027624294784101, 5.0276...  \n",
      "14 0.004119873046875 2.8669479098730797\n",
      "(0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "Q_learning\n",
      "1: 3.2419625985195655\n",
      "2: 3.45468712711018\n",
      "3: 2.9725909845507967\n",
      "4: 3.1199088702065234\n",
      "5: 1.1\n",
      "6: 3.4605589330321034\n",
      "7: 3.1957922700013963\n",
      "8: 0.8\n",
      "9: 3.3193457298294207\n",
      "10: 3.358040492544265\n",
      "11: 3.052963918650888\n",
      "12: 3.165223845408274\n",
      "13: 3.4057947654678107\n",
      "14: 3.4913868295783455\n",
      "15: 3.200831840749126\n",
      "16: 3.1403480871645533\n",
      "17: 3.3697538256736776\n",
      "18: 3.3341063241755533\n",
      "19: 2.8207445098572754\n",
      "20: 0.85\n",
      "21: 0.95\n",
      "22: 3.486291960372364\n",
      "23: 3.271429634595206\n",
      "24: 0.95\n",
      "25: 3.1392181548026974\n",
      "26: 3.4049870182429904\n",
      "27: 3.1145079958705306\n",
      "28: 2.9350139196004505\n",
      "29: 3.455509245021752\n",
      "30: 3.4166385707092233\n",
      "31: 3.3223197902165595\n",
      "32: 1.0\n",
      "0    True\n",
      "1    True\n",
      "2    True\n",
      "3    True\n",
      "4    True\n",
      "5    True\n",
      "Name: Policy, dtype: bool\n",
      "   Iterations  Alpha Decay  Alpha Min  Epsilon  Epsilon Decay    Reward  \\\n",
      "0     1000000        0.990     0.0010     10.0          0.990  3.241963   \n",
      "1     1000000        0.990     0.0001     10.0          0.990  3.454687   \n",
      "2     1000000        0.999     0.0010     10.0          0.990  2.972591   \n",
      "3     1000000        0.999     0.0001     10.0          0.990  3.119909   \n",
      "4     1000000        0.990     0.0010     10.0          0.999  1.100000   \n",
      "5     1000000        0.990     0.0001     10.0          0.999  3.460559   \n",
      "6     1000000        0.999     0.0010     10.0          0.999  3.195792   \n",
      "7     1000000        0.999     0.0001     10.0          0.999  0.800000   \n",
      "8     1000000        0.990     0.0010      1.0          0.990  3.319346   \n",
      "9     1000000        0.990     0.0001      1.0          0.990  3.358040   \n",
      "10    1000000        0.999     0.0010      1.0          0.990  3.052964   \n",
      "11    1000000        0.999     0.0001      1.0          0.990  3.165224   \n",
      "12    1000000        0.990     0.0010      1.0          0.999  3.405795   \n",
      "13    1000000        0.990     0.0001      1.0          0.999  3.491387   \n",
      "14    1000000        0.999     0.0010      1.0          0.999  3.200832   \n",
      "15    1000000        0.999     0.0001      1.0          0.999  3.140348   \n",
      "16   10000000        0.990     0.0010     10.0          0.990  3.369754   \n",
      "17   10000000        0.990     0.0001     10.0          0.990  3.334106   \n",
      "18   10000000        0.999     0.0010     10.0          0.990  2.820745   \n",
      "19   10000000        0.999     0.0001     10.0          0.990  0.850000   \n",
      "20   10000000        0.990     0.0010     10.0          0.999  0.950000   \n",
      "21   10000000        0.990     0.0001     10.0          0.999  3.486292   \n",
      "22   10000000        0.999     0.0010     10.0          0.999  3.271430   \n",
      "23   10000000        0.999     0.0001     10.0          0.999  0.950000   \n",
      "24   10000000        0.990     0.0010      1.0          0.990  3.139218   \n",
      "25   10000000        0.990     0.0001      1.0          0.990  3.404987   \n",
      "26   10000000        0.999     0.0010      1.0          0.990  3.114508   \n",
      "27   10000000        0.999     0.0001      1.0          0.990  2.935014   \n",
      "28   10000000        0.990     0.0010      1.0          0.999  3.455509   \n",
      "29   10000000        0.990     0.0001      1.0          0.999  3.416639   \n",
      "30   10000000        0.999     0.0010      1.0          0.999  3.322320   \n",
      "31   10000000        0.999     0.0001      1.0          0.999  1.000000   \n",
      "\n",
      "          Time                                             Policy  \\\n",
      "0    30.146454  (0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, ...   \n",
      "1    30.556892  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, ...   \n",
      "2    30.452300  (0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, ...   \n",
      "3    30.393956  (0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, ...   \n",
      "4    30.608307  (0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "5    31.347832  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "6    32.066239  (0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, ...   \n",
      "7    31.728552  (0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 0, ...   \n",
      "8    30.747011  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, ...   \n",
      "9    30.641253  (0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, ...   \n",
      "10   31.907561  (0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, ...   \n",
      "11   31.771899  (0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, ...   \n",
      "12   31.642624  (0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, ...   \n",
      "13   31.155180  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "14   31.677103  (0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, ...   \n",
      "15   30.904426  (0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, ...   \n",
      "16  317.467142  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "17  326.132012  (0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "18  325.441822  (0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...   \n",
      "19  327.039422  (0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, ...   \n",
      "20  333.530357  (0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, ...   \n",
      "21  340.253806  (0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "22  343.378037  (0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, ...   \n",
      "23  342.398696  (0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, ...   \n",
      "24  338.192452  (0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, ...   \n",
      "25  333.967251  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, ...   \n",
      "26  343.243903  (0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, ...   \n",
      "27  344.614648  (0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, ...   \n",
      "28  338.908743  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "29  372.666701  (0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "30  358.121638  (0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, ...   \n",
      "31  355.237210  (0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, ...   \n",
      "\n",
      "                                       Value Function  \\\n",
      "0   (4.469516616179936, 5.021068518048445, 5.02559...   \n",
      "1   (4.439242057739486, 4.991535438072977, 4.02348...   \n",
      "2   (4.484346439269151, 5.034010306485361, 5.03050...   \n",
      "3   (4.472149848246118, 5.025254217665293, 4.80726...   \n",
      "4   (4.475160791588194, 5.030119252396098, 5.03305...   \n",
      "5   (4.4320373705496445, 4.983831695810947, 4.0022...   \n",
      "6   (4.4755272438345965, 5.026659913734769, 5.0244...   \n",
      "7   (4.47387595179908, 5.02543011236457, 4.8995375...   \n",
      "8   (4.4715302899745675, 5.025254103920376, 5.0260...   \n",
      "9   (4.440501282624428, 4.9917634204759755, 4.0250...   \n",
      "10  (4.473243669077801, 5.024557765574801, 5.02822...   \n",
      "11  (4.474735106643947, 5.026734368971076, 4.79515...   \n",
      "12  (4.468336410499357, 5.024880214155668, 5.02617...   \n",
      "13  (4.437712559977625, 4.989278727892473, 4.05522...   \n",
      "14  (4.483947176287941, 5.027209497405266, 5.02235...   \n",
      "15  (4.4733679982784, 5.027344760662783, 4.9210824...   \n",
      "16  (4.478727097439459, 5.030297107574103, 5.02726...   \n",
      "17  (4.472545747925688, 5.026882228828553, 4.47434...   \n",
      "18  (4.487381826567785, 5.036270669291105, 5.03398...   \n",
      "19  (4.475654838515648, 5.027937255853335, 5.02832...   \n",
      "20  (4.478315670860931, 5.030859434325373, 5.03174...   \n",
      "21  (4.474343961653026, 5.026747473331291, 5.02772...   \n",
      "22  (4.47385368370041, 5.02501370437887, 5.0223674...   \n",
      "23  (4.473700196315319, 5.026368493160579, 5.02740...   \n",
      "24  (4.473987946896234, 5.027548079374795, 5.02555...   \n",
      "25  (4.473806835083367, 5.026323456779912, 5.02747...   \n",
      "26  (4.469272793002597, 5.021617837049628, 5.02717...   \n",
      "27  (4.470958536828379, 5.023586130976227, 5.02661...   \n",
      "28  (4.469424831306316, 5.023062253352429, 5.02330...   \n",
      "29  (4.472547871818648, 5.025404751471131, 5.02714...   \n",
      "30  (4.4748960340209925, 5.028188398473192, 5.0310...   \n",
      "31  (4.47285829170428, 5.025951108926845, 5.026750...   \n",
      "\n",
      "                                     Training Rewards  \n",
      "0   [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
      "1   [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, ...  \n",
      "2   [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "3   [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...  \n",
      "4   [6.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, ...  \n",
      "5   [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...  \n",
      "6   [10.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0,...  \n",
      "7   [0.0, 0.0, 0.0, 0.0, 1.0, 10.0, 0.0, 0.0, 0.0,...  \n",
      "8   [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 10.0, 0.0, 0.0,...  \n",
      "9   [0.0, 10.0, 0.0, 0.0, 10.0, 0.0, 0.0, 0.0, 1.0...  \n",
      "10  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
      "11  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
      "12  [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, ...  \n",
      "13  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "14  [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "15  [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...  \n",
      "16  [1.0, 0.0, 0.0, 1.0, 10.0, 1.0, 1.0, 1.0, 1.0,...  \n",
      "17  [0.0, 0.0, 1.0, 10.0, 0.0, 0.0, 0.0, 0.0, 0.0,...  \n",
      "18  [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "19  [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, ...  \n",
      "20  [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, ...  \n",
      "21  [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 10.0, 1.0, 0.0,...  \n",
      "22  [1.0, 0.0, 0.0, 10.0, 0.0, 0.0, 0.0, 1.0, 0.0,...  \n",
      "23  [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, ...  \n",
      "24  [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, ...  \n",
      "25  [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, ...  \n",
      "26  [0.0, 0.0, 1.0, 10.0, 10.0, 0.0, 1.0, 1.0, 1.0...  \n",
      "27  [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
      "28  [0.0, 10.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0,...  \n",
      "29  [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 10.0, 0.0, 1.0,...  \n",
      "30  [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...  \n",
      "31  [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, ...  \n",
      "            Alpha Decay  Alpha Min  Epsilon  Epsilon Decay    Reward  \\\n",
      "Iterations                                                             \n",
      "1000000          0.9945    0.00055      5.5         0.9945  2.967465   \n",
      "10000000         0.9945    0.00055      5.5         0.9945  2.676283   \n",
      "\n",
      "                  Time  \n",
      "Iterations              \n",
      "1000000      31.109224  \n",
      "10000000    340.037115  \n",
      "               Alpha Decay  Alpha Min  Epsilon    Reward        Time\n",
      "Epsilon Decay                                                       \n",
      "0.990               0.9945    0.00055      5.5  3.040816  181.419749\n",
      "0.999               0.9945    0.00055      5.5  2.602931  189.726591\n",
      "500 States\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Epsilon                                             Policy Iteration  \\\n",
      "0  1.000000e-01  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...        79   \n",
      "1  1.000000e-03  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       119   \n",
      "2  1.000000e-06  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       179   \n",
      "3  1.000000e-09  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       239   \n",
      "4  1.000000e-12  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       299   \n",
      "5  1.000000e-15  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...       349   \n",
      "\n",
      "       Time    Reward                                     Value Function  \n",
      "0  0.007670  2.725382  (4.710556185449387, 5.239434944489701, 5.23943...  \n",
      "1  0.012771  2.726020  (4.7117745667154995, 5.240595870281114, 5.2405...  \n",
      "2  0.021452  2.750285  (4.711792669916437, 5.240613400253226, 5.24061...  \n",
      "3  0.020985  2.755154  (4.711792702216012, 5.240613431989174, 5.24061...  \n",
      "4  0.026408  2.735265  (4.711792702273827, 5.240613432046434, 5.24061...  \n",
      "5  0.033520  2.743204  (4.7117927022739305, 5.240613432046538, 5.2406...  \n",
      "46 0.15735983848571777 2.7282127846897812\n",
      "(0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0)\n",
      "Q_learning\n",
      "1: 2.6457202687964307\n",
      "2: 2.649979984464296\n",
      "3: 2.6392875993822655\n",
      "4: 2.6315063673157337\n",
      "5: 2.6334844975422964\n",
      "6: 2.662812261587241\n",
      "7: 2.579535594724274\n",
      "8: 2.6408281271978216\n",
      "9: 2.615333529405131\n",
      "10: 0.854\n",
      "11: 2.6253267906693583\n",
      "12: 2.622869833551158\n",
      "13: 2.64128539283331\n",
      "14: 2.6388866548579846\n",
      "15: 2.6272875183965785\n",
      "16: 2.621154178848485\n",
      "17: 2.753957755761328\n",
      "18: 2.8307366870778288\n",
      "19: 2.770383137892614\n",
      "20: 2.8005122228172747\n",
      "21: 2.7597817847014805\n",
      "22: 2.8389751044306744\n",
      "23: 2.7384627338584178\n",
      "24: 2.8055208965447593\n",
      "25: 2.750629574729021\n",
      "26: 2.8216984597515604\n",
      "27: 2.758731007363096\n",
      "28: 2.7894710568318906\n",
      "29: 2.7627961451992804\n",
      "30: 2.831902914125298\n",
      "31: 2.766254934955373\n",
      "32: 2.781121279609314\n",
      "0    True\n",
      "1    True\n",
      "2    True\n",
      "3    True\n",
      "4    True\n",
      "5    True\n",
      "Name: Policy, dtype: bool\n",
      "   Iterations  Alpha Decay  Alpha Min  Epsilon  Epsilon Decay    Reward  \\\n",
      "0     1000000        0.990     0.0010     10.0          0.990  2.645720   \n",
      "1     1000000        0.990     0.0001     10.0          0.990  2.649980   \n",
      "2     1000000        0.999     0.0010     10.0          0.990  2.639288   \n",
      "3     1000000        0.999     0.0001     10.0          0.990  2.631506   \n",
      "4     1000000        0.990     0.0010     10.0          0.999  2.633484   \n",
      "5     1000000        0.990     0.0001     10.0          0.999  2.662812   \n",
      "6     1000000        0.999     0.0010     10.0          0.999  2.579536   \n",
      "7     1000000        0.999     0.0001     10.0          0.999  2.640828   \n",
      "8     1000000        0.990     0.0010      1.0          0.990  2.615334   \n",
      "9     1000000        0.990     0.0001      1.0          0.990  0.854000   \n",
      "10    1000000        0.999     0.0010      1.0          0.990  2.625327   \n",
      "11    1000000        0.999     0.0001      1.0          0.990  2.622870   \n",
      "12    1000000        0.990     0.0010      1.0          0.999  2.641285   \n",
      "13    1000000        0.990     0.0001      1.0          0.999  2.638887   \n",
      "14    1000000        0.999     0.0010      1.0          0.999  2.627288   \n",
      "15    1000000        0.999     0.0001      1.0          0.999  2.621154   \n",
      "16   10000000        0.990     0.0010     10.0          0.990  2.753958   \n",
      "17   10000000        0.990     0.0001     10.0          0.990  2.830737   \n",
      "18   10000000        0.999     0.0010     10.0          0.990  2.770383   \n",
      "19   10000000        0.999     0.0001     10.0          0.990  2.800512   \n",
      "20   10000000        0.990     0.0010     10.0          0.999  2.759782   \n",
      "21   10000000        0.990     0.0001     10.0          0.999  2.838975   \n",
      "22   10000000        0.999     0.0010     10.0          0.999  2.738463   \n",
      "23   10000000        0.999     0.0001     10.0          0.999  2.805521   \n",
      "24   10000000        0.990     0.0010      1.0          0.990  2.750630   \n",
      "25   10000000        0.990     0.0001      1.0          0.990  2.821698   \n",
      "26   10000000        0.999     0.0010      1.0          0.990  2.758731   \n",
      "27   10000000        0.999     0.0001      1.0          0.990  2.789471   \n",
      "28   10000000        0.990     0.0010      1.0          0.999  2.762796   \n",
      "29   10000000        0.990     0.0001      1.0          0.999  2.831903   \n",
      "30   10000000        0.999     0.0010      1.0          0.999  2.766255   \n",
      "31   10000000        0.999     0.0001      1.0          0.999  2.781121   \n",
      "\n",
      "          Time                                             Policy  \\\n",
      "0    51.240564  (0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, ...   \n",
      "1    49.933125  (0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, ...   \n",
      "2    49.678569  (0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, ...   \n",
      "3    50.765998  (0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, ...   \n",
      "4    48.140704  (0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, ...   \n",
      "5    52.764103  (0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, ...   \n",
      "6    48.051532  (0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, ...   \n",
      "7    52.965909  (0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, ...   \n",
      "8    50.208746  (0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "9    48.248877  (0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, ...   \n",
      "10   47.552881  (0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n",
      "11   49.738924  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, ...   \n",
      "12   48.128707  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, ...   \n",
      "13   48.746337  (0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, ...   \n",
      "14   48.805255  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, ...   \n",
      "15   49.272156  (0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, ...   \n",
      "16  481.452286  (0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "17  466.284662  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "18  480.409786  (0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "19  990.285389  (0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "20  508.476199  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, ...   \n",
      "21  496.778739  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "22  491.540097  (0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, ...   \n",
      "23  510.654509  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "24  521.783642  (0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "25  457.691807  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "26  460.544369  (0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "27  460.243234  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, ...   \n",
      "28  460.271363  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, ...   \n",
      "29  480.049701  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "30  504.557330  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, ...   \n",
      "31  544.296999  (0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
      "\n",
      "                                       Value Function  \\\n",
      "0   (4.711608976583683, 5.240736056301092, 5.24235...   \n",
      "1   (4.672435445357042, 5.201171633515619, 4.37127...   \n",
      "2   (4.7127953406989, 5.240646359078045, 5.2397861...   \n",
      "3   (4.709399081256882, 5.2385485235715645, 5.0989...   \n",
      "4   (4.710522511015649, 5.239877278961145, 5.23978...   \n",
      "5   (4.667861470968527, 5.196229858663855, 4.36511...   \n",
      "6   (4.713127913126084, 5.24227256659081, 5.241421...   \n",
      "7   (4.708568498848688, 5.237654220967237, 5.13409...   \n",
      "8   (4.709616563111147, 5.239579907160608, 5.24055...   \n",
      "9   (4.671803722165624, 5.200420854442883, 4.37439...   \n",
      "10  (4.708798041638176, 5.237348262835154, 5.23897...   \n",
      "11  (4.710611883592539, 5.239766727493796, 5.07795...   \n",
      "12  (4.71104961560227, 5.240555934341656, 5.240119...   \n",
      "13  (4.667983982759229, 5.196549386529219, 4.33630...   \n",
      "14  (4.71420263693811, 5.241776361592537, 5.241448...   \n",
      "15  (4.70893265533937, 5.237969010334612, 5.135133...   \n",
      "16  (4.715240039024286, 5.243235776603333, 5.24187...   \n",
      "17  (4.711408241514055, 5.240331992175588, 5.24084...   \n",
      "18  (4.710333198733517, 5.238904513109211, 5.24051...   \n",
      "19  (4.7112519988621555, 5.24024930807696, 5.24036...   \n",
      "20  (4.714717195027458, 5.242958034955538, 5.24098...   \n",
      "21  (4.711973921873225, 5.240576948153227, 5.24040...   \n",
      "22  (4.712788341687959, 5.240693773673025, 5.23930...   \n",
      "23  (4.711497235915598, 5.2403455492798106, 5.2407...   \n",
      "24  (4.7103739865486585, 5.239823353197184, 5.2408...   \n",
      "25  (4.712662839054791, 5.241339604775162, 5.24059...   \n",
      "26  (4.713983234416544, 5.242644688779662, 5.24156...   \n",
      "27  (4.711567622550278, 5.240700337742658, 5.24075...   \n",
      "28  (4.708659551509187, 5.237944930089279, 5.23920...   \n",
      "29  (4.711861786864244, 5.240179461218474, 5.24015...   \n",
      "30  (4.710272547721519, 5.2395490115711025, 5.2405...   \n",
      "31  (4.7117728053645465, 5.240668420637552, 5.2405...   \n",
      "\n",
      "                                     Training Rewards  \n",
      "0   [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "1   [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "2   [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, ...  \n",
      "3   [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
      "4   [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, ...  \n",
      "5   [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "6   [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "7   [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "8   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
      "9   [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "10  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "11  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
      "12  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "13  [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "14  [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "15  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "16  [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
      "17  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "18  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "19  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "20  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
      "21  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 100.0...  \n",
      "22  [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "23  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
      "24  [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
      "25  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
      "26  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, ...  \n",
      "27  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
      "28  [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
      "29  [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, ...  \n",
      "30  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...  \n",
      "31  [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, ...  \n",
      "            Alpha Decay  Alpha Min  Epsilon  Epsilon Decay    Reward  \\\n",
      "Iterations                                                             \n",
      "1000000          0.9945    0.00055      5.5         0.9945  2.520581   \n",
      "10000000         0.9945    0.00055      5.5         0.9945  2.785058   \n",
      "\n",
      "                  Time  \n",
      "Iterations              \n",
      "1000000      49.640149  \n",
      "10000000    519.707507  \n",
      "               Alpha Decay  Alpha Min  Epsilon    Reward        Time\n",
      "Epsilon Decay                                                       \n",
      "0.990               0.9945    0.00055      5.5  2.597509  294.753929\n",
      "0.999               0.9945    0.00055      5.5  2.708131  274.593728\n"
     ]
    }
   ],
   "source": [
    "def iteration_over_episode(probability_matrix, reward_matrix, policy, j, episodes, cumulative_reward, iterations_per_state=1000, gamma=0.9):\n",
    "    iterative_reward = 0\n",
    "    for tmp in range(iterations_per_state):\n",
    "        r = 0\n",
    "        discount = 1\n",
    "        while True:\n",
    "            # take step\n",
    "            i = policy[j]\n",
    "            # get next step using probability_matrix\n",
    "            chance = probability_matrix[i][j]\n",
    "            a = list(range(len(probability_matrix[i][j])))\n",
    "            s_prime =  choice(a, 1, p=chance)[0]\n",
    "            # get the score\n",
    "            r_delta = reward_matrix[j][i] * discount\n",
    "            discount *= gamma\n",
    "            r += r_delta\n",
    "            if s_prime == 0:\n",
    "                break\n",
    "        iterative_reward += r\n",
    "\n",
    "    return iterative_reward\n",
    "\n",
    "def iteration_over_state(probability_matrix, reward_matrix, policy, total_states, episodes, cumulative_reward, iterations_per_state=1000, gamma=0.9):\n",
    "    for j in range(total_states):\n",
    "        iterative_reward = 0\n",
    "\n",
    "        iterative_reward = iteration_over_episode(probability_matrix, reward_matrix, policy, j, episodes, cumulative_reward, iterations_per_state, gamma)\n",
    "        \n",
    "        cumulative_reward += iterative_reward\n",
    "\n",
    "    return cumulative_reward\n",
    "\n",
    "\n",
    "def testing(probability_matrix, reward_matrix, policy, iterations_per_state=1000, gamma=0.9):\n",
    "    total_states = probability_matrix.shape[-1]\n",
    "    episodes = total_states * iterations_per_state\n",
    "\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    cumulative_reward = iteration_over_state(probability_matrix, reward_matrix, policy, total_states, episodes, cumulative_reward, iterations_per_state, gamma)\n",
    "    \n",
    "    return cumulative_reward / episodes\n",
    "\n",
    "def value_iteration(probability_matrix, reward_matrix, epsilon, gamma=0.9):\n",
    "    value_iteration_data_frame = pd.DataFrame(columns=[\"Epsilon\", \"Policy\", \"Iteration\", \"Time\", \"Reward\", \"Value Function\"])\n",
    "    for i in epsilon:\n",
    "        value_iteration = ValueIteration(probability_matrix, reward_matrix, gamma=gamma, epsilon=i, max_iter=int(1e15))\n",
    "        value_iteration.run()\n",
    "        r = testing(probability_matrix, reward_matrix, value_iteration.policy)\n",
    "        size = len(value_iteration_data_frame)\n",
    "        value_iteration_data_frame.loc[size] = [float(i), value_iteration.policy, value_iteration.iter, value_iteration.time, r, value_iteration.V]\n",
    "    return value_iteration_data_frame\n",
    "\n",
    "def Q_learning(probability_matrix, reward_matrix, gamma=0.9, learning_rate_decay=[0.99], learning_rate_cut_off=[0.001], epsilon=[1.0], epsilon_decay=[0.99], episodes=[1000000]):\n",
    "    Q_learning_data_frame = pd.DataFrame(columns=[\"Iterations\", \"Alpha Decay\", \"Alpha Min\", \"Epsilon\", \"Epsilon Decay\", \"Reward\", \"Time\", \"Policy\", \"Value Function\", \"Training Rewards\"])\n",
    "    \n",
    "    total = 0\n",
    "    for i in episodes:\n",
    "        for j in epsilon:\n",
    "            for k in epsilon_decay:\n",
    "                for learning_rate_d in learning_rate_decay:\n",
    "                    for learning_rate_m in learning_rate_cut_off:\n",
    "                        algo = QLearning(probability_matrix, reward_matrix, gamma, alpha_decay=learning_rate_d, \n",
    "                                      alpha_min=learning_rate_m, epsilon=j, \n",
    "                                      epsilon_decay=k, n_iter=i)\n",
    "                        algo.run()\n",
    "                        score = testing(probability_matrix, reward_matrix, algo.policy)\n",
    "                        total += 1\n",
    "                        print(\"{}: {}\".format(total, score))\n",
    "                        information = algo.run_stats\n",
    "                        scores = [tmp['Reward'] for tmp in information]\n",
    "                        \n",
    "                        size = len(Q_learning_data_frame)\n",
    "                        Q_learning_data_frame.loc[size] = [i, learning_rate_d, learning_rate_m, j, k, score, algo.time, algo.policy, algo.V, scores]\n",
    "\n",
    "    return Q_learning_data_frame\n",
    "\n",
    "\n",
    "def run_forest_management(probability_matrix, reward_matrix):\n",
    "    value_iteration_data_frame = value_iteration(probability_matrix, reward_matrix, epsilon=[1e-1, 1e-3, 1e-6, 1e-9, 1e-12, 1e-15])\n",
    "    print(value_iteration_data_frame)\n",
    "    \n",
    "    policy_iteration = PolicyIteration(probability_matrix, reward_matrix, gamma=0.9, max_iter=1e6)\n",
    "    policy_iteration.run()\n",
    "    policy_iteration_policy = policy_iteration.policy\n",
    "    policy_iteration_score = testing(probability_matrix, reward_matrix, policy_iteration_policy)\n",
    "    policy_iteration_episodes = policy_iteration.iter\n",
    "    policy_iteration_period = policy_iteration.time\n",
    "    print(policy_iteration_episodes, policy_iteration_period, policy_iteration_score)\n",
    "    \n",
    "    print(policy_iteration_policy)\n",
    "    \n",
    "    \n",
    "    print(\"Q_learning\")\n",
    "    \n",
    "    learning_rate_decay = [0.99, 0.999]\n",
    "    learning_rate_cut_off =[0.001, 0.0001]\n",
    "    i = [10.0, 1.0]\n",
    "    k = [0.99, 0.999]\n",
    "    episodes = [1000000, 10000000]\n",
    "    Q_learning_data_frame = Q_learning(probability_matrix, reward_matrix, gamma=0.9, learning_rate_decay=learning_rate_decay, learning_rate_cut_off=learning_rate_cut_off, epsilon=i, epsilon_decay=k, episodes=episodes)\n",
    "    \n",
    "    print(value_iteration_data_frame.Policy == policy_iteration_policy)\n",
    "    \n",
    "    testing(probability_matrix,reward_matrix, Q_learning_data_frame.Policy[18])\n",
    "    \n",
    "    print(Q_learning_data_frame)\n",
    "    \n",
    "    print(Q_learning_data_frame.groupby(\"Iterations\").mean())\n",
    "    \n",
    "    print(Q_learning_data_frame.groupby(\"Epsilon Decay\").mean())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(44)\n",
    "    \n",
    "    print(\"20 States\\n\\n\\n\")\n",
    "    \n",
    "    probability_matrix, reward_matrix = forest(S=20, r1=10, r2=6, p=0.1)\n",
    "\n",
    "    run_forest_management(probability_matrix, reward_matrix)\n",
    "    \n",
    "    print(\"500 States\\n\\n\\n\")\n",
    "    \n",
    "    probability_matrix, reward_matrix = forest(S=500, r1=100, r2= 15, p=0.01)\n",
    "    \n",
    "    run_forest_management(probability_matrix, reward_matrix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2dbf5b1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20 States\n",
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Epsilon</th>\n",
       "      <th>Policy</th>\n",
       "      <th>Iteration</th>\n",
       "      <th>Time</th>\n",
       "      <th>Reward</th>\n",
       "      <th>Value Function</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000e-01</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000624</td>\n",
       "      <td>26.085776</td>\n",
       "      <td>(8.1, 18.1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.000000e-03</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000099</td>\n",
       "      <td>25.813423</td>\n",
       "      <td>(8.1, 18.1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.000000e-06</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000095</td>\n",
       "      <td>25.116442</td>\n",
       "      <td>(8.1, 18.1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000e-09</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000189</td>\n",
       "      <td>26.793506</td>\n",
       "      <td>(8.1, 18.1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.000000e-12</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>27.398591</td>\n",
       "      <td>(8.1, 18.1)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.000000e-15</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>5</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>25.700052</td>\n",
       "      <td>(27.855900000000005, 37.855900000000005)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Epsilon  Policy Iteration      Time     Reward  \\\n",
       "0  1.000000e-01  (0, 0)         2  0.000624  26.085776   \n",
       "1  1.000000e-03  (0, 0)         2  0.000099  25.813423   \n",
       "2  1.000000e-06  (0, 0)         2  0.000095  25.116442   \n",
       "3  1.000000e-09  (0, 0)         2  0.000189  26.793506   \n",
       "4  1.000000e-12  (0, 0)         2  0.000150  27.398591   \n",
       "5  1.000000e-15  (0, 0)         5  0.000208  25.700052   \n",
       "\n",
       "                             Value Function  \n",
       "0                               (8.1, 18.1)  \n",
       "1                               (8.1, 18.1)  \n",
       "2                               (8.1, 18.1)  \n",
       "3                               (8.1, 18.1)  \n",
       "4                               (8.1, 18.1)  \n",
       "5  (27.855900000000005, 37.855900000000005)  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration\n",
      "1 0.0005059242248535156 26.618619308890423\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q_learning\n",
      "1: 26.52112820664776\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Iterations</th>\n",
       "      <th>Alpha Decay</th>\n",
       "      <th>Alpha Min</th>\n",
       "      <th>Epsilon</th>\n",
       "      <th>Epsilon Decay</th>\n",
       "      <th>Reward</th>\n",
       "      <th>Time</th>\n",
       "      <th>Policy</th>\n",
       "      <th>Value Function</th>\n",
       "      <th>Training Rewards</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000000</td>\n",
       "      <td>0.99</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>26.521128</td>\n",
       "      <td>30.639653</td>\n",
       "      <td>(0, 0)</td>\n",
       "      <td>(81.00347166855782, 91.02839025115826)</td>\n",
       "      <td>[0.0, 10.0, 6.0, 0.0, 0.0, 10.0, 0.0, 10.0, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Iterations  Alpha Decay  Alpha Min  Epsilon  Epsilon Decay     Reward  \\\n",
       "0    1000000         0.99      0.001      1.0           0.99  26.521128   \n",
       "\n",
       "        Time  Policy                          Value Function  \\\n",
       "0  30.639653  (0, 0)  (81.00347166855782, 91.02839025115826)   \n",
       "\n",
       "                                    Training Rewards  \n",
       "0  [0.0, 10.0, 6.0, 0.0, 0.0, 10.0, 0.0, 10.0, 0....  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alpha Decay</th>\n",
       "      <th>Alpha Min</th>\n",
       "      <th>Epsilon</th>\n",
       "      <th>Epsilon Decay</th>\n",
       "      <th>Reward</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Iterations</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000000</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>26.521128</td>\n",
       "      <td>30.639653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Alpha Decay  Alpha Min  Epsilon  Epsilon Decay     Reward  \\\n",
       "Iterations                                                              \n",
       "1000000            0.99      0.001      1.0           0.99  26.521128   \n",
       "\n",
       "                 Time  \n",
       "Iterations             \n",
       "1000000     30.639653  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Alpha Decay</th>\n",
       "      <th>Alpha Min</th>\n",
       "      <th>Epsilon</th>\n",
       "      <th>Reward</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Epsilon Decay</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.99</th>\n",
       "      <td>0.99</td>\n",
       "      <td>0.001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>26.521128</td>\n",
       "      <td>30.639653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Alpha Decay  Alpha Min  Epsilon     Reward       Time\n",
       "Epsilon Decay                                                       \n",
       "0.99                  0.99      0.001      1.0  26.521128  30.639653"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 States\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def iteration_over_episode(probability_matrix, reward_matrix, policy, j, episodes, cumulative_reward, iterations_per_state=1000, gamma=0.9):\n",
    "    iterative_reward = 0\n",
    "    for tmp in range(iterations_per_state):\n",
    "        r = 0\n",
    "        discount = 1\n",
    "        while True:\n",
    "            # take step\n",
    "            i = policy[j]\n",
    "            # get next step using probability_matrix\n",
    "            chance = probability_matrix[i][j]\n",
    "            a = list(range(len(probability_matrix[i][j])))\n",
    "            s_prime =  choice(a, 1, p=chance)[0]\n",
    "            # get the score\n",
    "            r_delta = reward_matrix[j][i] * discount\n",
    "            discount *= gamma\n",
    "            r += r_delta\n",
    "            if s_prime == 0:\n",
    "                break\n",
    "        iterative_reward += r\n",
    "\n",
    "    return iterative_reward\n",
    "\n",
    "def iteration_over_state(probability_matrix, reward_matrix, policy, total_states, episodes, cumulative_reward, iterations_per_state, gamma):\n",
    "    for j in range(total_states):\n",
    "        iterative_reward = 0\n",
    "\n",
    "        iterative_reward = iteration_over_episode(probability_matrix, reward_matrix, policy, j, episodes, cumulative_reward, iterations_per_state, gamma)\n",
    "        \n",
    "        cumulative_reward += iterative_reward\n",
    "\n",
    "    return cumulative_reward\n",
    "\n",
    "\n",
    "def testing(probability_matrix, reward_matrix, policy, iterations_per_state=1000, gamma=0.9):\n",
    "    total_states = probability_matrix.shape[-1]\n",
    "    episodes = total_states * iterations_per_state\n",
    "\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    cumulative_reward = iteration_over_state(probability_matrix, reward_matrix, policy, total_states, episodes, cumulative_reward, iterations_per_state, gamma)\n",
    "    \n",
    "    return cumulative_reward / episodes\n",
    "\n",
    "def value_iteration(probability_matrix, reward_matrix, epsilon, gamma=0.9):\n",
    "    value_iteration_data_frame = pd.DataFrame(columns=[\"Epsilon\", \"Policy\", \"Iteration\", \"Time\", \"Reward\", \"Value Function\"])\n",
    "    for i in epsilon:\n",
    "        value_iteration = ValueIteration(probability_matrix, reward_matrix, gamma=gamma, epsilon=i, max_iter=int(1e15))\n",
    "        value_iteration.run()\n",
    "        r = testing(probability_matrix, reward_matrix, value_iteration.policy)\n",
    "        value_iteration_data_frame.loc[len(value_iteration_data_frame)] = [float(i), value_iteration.policy, value_iteration.iter, value_iteration.time, r, value_iteration.V]\n",
    "    return value_iteration_data_frame\n",
    "\n",
    "def Q_learning(probability_matrix, reward_matrix, gamma=0.9, learning_rate_decay=[0.99], learning_rate_cut_off=[0.001], epsilon=[1.0], epsilon_decay=[0.99], episodes=[1000000]):\n",
    "    Q_learning_data_frame = pd.DataFrame(columns=[\"Iterations\", \"Alpha Decay\", \"Alpha Min\", \"Epsilon\", \"Epsilon Decay\", \"Reward\", \"Time\", \"Policy\", \"Value Function\", \"Training Rewards\"])\n",
    "    \n",
    "    total = 0\n",
    "    for i in episodes:\n",
    "        for j in epsilon:\n",
    "            for k in epsilon_decay:\n",
    "                for learning_rate_d in learning_rate_decay:\n",
    "                    for learning_rate_m in learning_rate_cut_off:\n",
    "                        algo = QLearning(probability_matrix, reward_matrix, gamma, alpha_decay=learning_rate_d, alpha_min=learning_rate_m, epsilon=j, epsilon_decay=k, n_iter=i)\n",
    "                        algo.run()\n",
    "                        score = testing(probability_matrix, reward_matrix, algo.policy)\n",
    "                        total += 1\n",
    "                        print(\"{}: {}\".format(total, score))\n",
    "                        scores = [tmp['Reward'] for tmp in algo.run_stats]\n",
    "                        \n",
    "                        Q_learning_data_frame.loc[len(Q_learning_data_frame)] = [i, learning_rate_d, learning_rate_m, j, k, score, algo.time, algo.policy, algo.V, scores]\n",
    "\n",
    "    return Q_learning_data_frame\n",
    "\n",
    "def run_policy_iteration(probability_matrix, reward_matrix):\n",
    "    print(\"Policy Iteration\")\n",
    "\n",
    "    policy_iteration = PolicyIteration(probability_matrix, reward_matrix, gamma=0.9, max_iter=1e6)\n",
    "    policy_iteration.run()\n",
    "    policy_iteration_policy = policy_iteration.policy\n",
    "    policy_iteration_score = testing(probability_matrix, reward_matrix, policy_iteration_policy)\n",
    "    print(policy_iteration.iter, policy_iteration.time, policy_iteration_score)\n",
    "    \n",
    "    display(policy_iteration_policy)\n",
    "\n",
    "\n",
    "def run_forest_management(probability_matrix, reward_matrix):\n",
    "    value_iteration_data_frame = value_iteration(probability_matrix, reward_matrix, epsilon=[1e-1, 1e-3, 1e-6, 1e-9, 1e-12, 1e-15])\n",
    "    display(value_iteration_data_frame)\n",
    "    \n",
    "    run_policy_iteration(probability_matrix, reward_matrix)\n",
    "    \n",
    "    \n",
    "    print(\"Q_learning\")\n",
    "    \n",
    "    learning_rate_decay = [0.99, 0.999]\n",
    "    learning_rate_cut_off =[0.001, 0.0001]\n",
    "    i = [10.0, 1.0]\n",
    "    k = [0.99, 0.999]\n",
    "    episodes = [1000000, 10000000]\n",
    "    Q_learning_data_frame = Q_learning(probability_matrix, reward_matrix, gamma=0.9, learning_rate_decay=learning_rate_decay, learning_rate_cut_off=learning_rate_cut_off, epsilon=i, epsilon_decay=k, episodes=episodes)\n",
    "    \n",
    "    \n",
    "    \n",
    "    testing(probability_matrix,reward_matrix, Q_learning_data_frame.Policy[18])\n",
    "    \n",
    "    display(Q_learning_data_frame)\n",
    "    \n",
    "    display(Q_learning_data_frame.groupby(\"Iterations\").mean())\n",
    "    \n",
    "    display(Q_learning_data_frame.groupby(\"Epsilon Decay\").mean())\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    np.random.seed(44)\n",
    "    \n",
    "    print(\"20 States\\n\\n\\n\")\n",
    "    \n",
    "    probability_matrix, reward_matrix = forest(S=2, r1=10, r2=6, p=0.1)\n",
    "\n",
    "    run_forest_management(probability_matrix, reward_matrix)\n",
    "    \n",
    "    print(\"500 States\\n\\n\\n\")\n",
    "    \n",
    "    probability_matrix, reward_matrix = forest(S=500, r1=100, r2= 15, p=0.01)\n",
    "    \n",
    "    run_forest_management(probability_matrix, reward_matrix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c2d698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
